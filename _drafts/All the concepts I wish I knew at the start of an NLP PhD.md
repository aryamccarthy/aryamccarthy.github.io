---
layout: post
---

# All the concepts I wish I knew at the start of an NLP PhD

- Embeddings
- Language model (nebulous)
- Kullback–Leibler divergence
- Softmax
- Gibbs sampling in NLP
- Expectation–Maximization (EM)—more than just counting things
- Generative model (vs discriminative)
- Model (it's overloaded. The shape of the model, or its particular parameters.)
- Generative adversarial network (GAN)
- LSTM
- RNN vs CNN vs DNN (all the NNs)
- Fully differentiable (and why)
- Gumbel–Softmax trick (maaaaybe)
- Logits
- Attention—a learned weighted sum of vectors
- MLP (maaaybe)
- Neurolinguistic programming
- The grid
