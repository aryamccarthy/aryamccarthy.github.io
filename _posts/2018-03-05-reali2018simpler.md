---
layout: paper
title: "Simpler Grammar, Larger Vocabulary: How Population Size Affects Language"
authors: "Reali et al."
year: 2018
venue: "Proceedings of the Royal Society B"
link: "http://dx.doi.org/10.1098/rspb.2017.2586"
---

The title makes this paper's message pretty clear. When the speaker pool of a language gets large, easy-to-transmit information like vocabulary grows, while harder ideas like grammar tend do simplify. The authors present a network science model to show the diffusion of these concepts. With it, they conjecture that globalization will engender a simplification of grammatical structure.

<!--more-->

Forming complex linguistic features require a consensus, as in what emerges between tight-knit societies. Irregularities, morphology, and agreement need to be incubated, or they'll stop being transmitted. Languages simplify as their number of speakers grows. Some have attributed this to the increase in [L2 speakers](https://en.wikipedia.org/wiki/Second_language). On the other hand, more speakers makes a richer vocabulary (of content words) possible. More people have more spheres they need to conceptualize and describe.

Another proposal for why these changes develop as they do is that adult–child interactions convey morphology and syntax, while adult–adult interactions allow "lexical innovations". It's also easier for these lexical innovations to cross from one language into another ([borrowings]({{ "/ciobanu2015automatic" | absolute_url }})).

These authors have a "parsimonious alternative": that both lexical and structural complexity depend only on the ease of diffusion. I can certainly attest from my experiences using Duolingo for Chinese that it's easier to pick up words than grammar, given only a few exposures.

---

To test their alternative hypothesis, they first manually divide properties into two categories: those that are *Easy* and *Hard*. (They also point out relations to how musical or dance styles like lindy hop similarly emerged from tight-knit groups. On the other hand, [pop music is getting simpler](https://pudding.cool/2017/05/song-repetition/).) These concepts are then disseminated using their innovate-and-propagate model from network science, which is like a noisy version of the [label propagation](https://en.wikipedia.org/wiki/Label_Propagation_Algorithm) process, with the noise in the form of a Chinese Restaurant Process. CRPs come up everywhere, so check out [Kevin Knight's explanation]({{ "/knight2009bayesian" | absolute_url }}) of them. Their key idea is that the rich get richer. Popular ideas are more likely to be picked.

The "tables" in our Chinese restaurant are linguistic phenomena. Either you take one of the conventions you've seen before, or with a small (and diminishing) probability, you come up with a new innovation to spread around. (They assigned a 50/50 chance that innovations would be *Hard*.) The label propagation side of this is that agents pick from their list of symbols (representing lingusitic concepts) or a new symbol, then also pick a random neighbor and send that symbol to the random neighbor.

The authors implement two rather interesting models which better model real-world speakers. One is "immortal-but-forgetful" speakers. At each turn, it may forget each of its symbols with some probability. The other is a "dying-off" model: nodes drop out of the network to be replaced with blank slates. Finally, *Easy* and *Hard* are incorporated by deciding that an agent has only learned a *Hard* convention when it's been seen *twice*.

They picked a network model (without name) tht varies the number of nodes, the average degree, and the clustering coefficient. (Sounds like a [Holme and Kim graph generator](https://networkx.github.io/documentation/latest/reference/generated/networkx.generators.random_graphs.powerlaw_cluster_graph.html).) Unfortunately, their sampling method seems really misguided. It uses Metropolis–Hastings, which is a Markov Chain Monte Carlo method. They took only 25 samples from it, then picked the 5 networks that best matched their target parameters. 

> Never trust a sample size under 30
> —Jason Eisner.

The findings are actually really interesting, though they're also exactly the trends you'd expect going into this. (Maybe that's the sign of a good paper—outlining your process so well that the results seem trivial.) But even without this paper, you'd suspect that with a large population, fewer neighbors would share conventions.

One question that the paper glosses over is the notion of what a language *is*. If a limited group of speakers use unique vocabulary, is it a different language? Most people would call it a dialect, looking at American and British English. What if they have slightly different grammars? You have this example in Farsi and Tajik, where they pronounce vowels differently, plus use a different auxiliary verb to form the present progressive. Or perhaps we can settle that ["A language is a dialect with an army and a navy."](https://en.wikipedia.org/wiki/A_language_is_a_dialect_with_an_army_and_navy)

Further, plots keep showing the Hard conventions' prevalence divebombing toward zero. Are Reali et al. going to claim that we're moving toward a state of zero grammar? Or do they need to enforce greater "stickiness" in grammatical rules than vocabulary, by severely dropping their probability of forgetting?

**The bottom line:**

- Simple lingustic innovations propagate more widely than complicated ones, as the population increases.
