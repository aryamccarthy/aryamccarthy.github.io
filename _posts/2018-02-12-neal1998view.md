---
title: "A View of the EM Algorithm that Justifies Incremental, Sparse, and Other Variants"
layout: post
excerpt_separator: <!--more-->
---

([Neal and Hinton 1998](ftp.cs.utoronto.ca/cs/ftp/pub/radford/emk.pdf)) in _Learning in Graphical Models_, ed. Michael I. Jordan.



<!--more-->

Expectation–Maximization (EM) is an algorithm that performs alternating optimization. It starts at an initial guess of a model's parameters, then alternately finds distributions of the unknown variables (E-step), then re-estimates the parametrs to maximize the likelihood (M-step). 

There are variants of EM where the M-step doesn't maximize, but still improves, the likelihood. The E-step can also be partial, and this paper is the first to formally treat that style of EM.

EM can be seen as maximizing the free energy: the E-step maximizes it with respect to latent variables, and M-step maximizes it with respect to the parameters.

Let's say that we've observed **_Z_**, but not **_Y_**. Their joint distribution is parameterized by **_θ_**: we're trying to maximize the log-likelihood of the data: **_L_(_θ_) = log P(_z_|_θ_)**. Both regular and partial EM are guaranteed to never diminish this quantity.


