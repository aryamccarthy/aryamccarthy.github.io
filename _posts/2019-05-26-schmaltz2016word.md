---
layout: paper
title: Word Ordering Without Syntax
authors: Schmaltz, Rush, and Shieber
year: 2016
venue: EMNLP
link: https://aclweb.org/anthology/D16-1255
---

This paper was part of the wave that showed how recurrent neural nets could be applied to heaps of tasks. 

<!--more-->

## Problem

The problem is pretty straightforward. You're given the words and punctuation of a sentence, scrambled. Instead of "*Papa ate the caviar with a spoon .*", you get "*. ate with the a caviar spoon Papa*"[^a]â€”utterly unintelligible. The challenge is to rediscover the original order. 

What makes this challenging is that a sentence of \\(N\\) words has \\(N!\\) possible arrangements. It's NP-hard to evaluate each arrangement individually by brute force, to identify the highest quality.

## Approach

The authors believe that explicit syntactic information isn't the key to reordering. Instead, they claim that the best ordering should be the most probable one according to a language model. They propose two language models: an n-gram model and an LSTM.


## Data

The authors try two task settings:

Words
: Words all provided independently of each other.
Words+BNPs
: Base noun phrases (those without other noun phrases inside) are also provided.

## Results

## Analysis

## Conclusion  


[^a]: Courtesy of this `bash` pipeline: `echo "Papa ate the caviar with a spoon ." | tr ' ' '\n' | sort -R | paste -s -d ' ' -`

*LSTM: 
