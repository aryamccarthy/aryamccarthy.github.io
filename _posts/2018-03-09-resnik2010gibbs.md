---
layout: paper
title: "Gibbs Sampling for the Uninitiated"
authors: "Resnik and Hardisty"
year: 2010
link: "https://drum.lib.umd.edu/bitstream/handle/1903/10058/gsfu.pdf?sequence=3"
mathjax: true
---

Kevin Knight's tutorial on Bayesian methods is one of the most approachable, human pieces of writing to describe a highfalutin concept. This technical report from the University of Maryland at College Park applies that style to Gibbs sampling. It's less afraid to introduce maths, and it still grounds the problems in our intuitions as NLP researchers.

--- 

The first idea that we're exposed to is the difference between maximum likelihood estimation (MLE) and maximum a posteriori estimation (MAP). The latter seeks the `argmax` of the posterior, which is equivalent to the argmax of the likelihood times the prior. 

\begin{align}
\hat{\pi}\_{MLE} & = \arg\max\_\pi \Pr(\mathcal{X} | \pi) \\
\Pr(y | \mathcal{X}) & \approx P(y | \hat{pi}\_{MLE})
\end{align}

\begin{align}
\sqrt{37} & = \sqrt{\frac{73^2-1}{12^2}} \\
 & = \sqrt{\frac{73^2}{12^2}\cdot\frac{73^2-1}{73^2}} \\ 
 & = \sqrt{\frac{73^2}{12^2}}\sqrt{\frac{73^2-1}{73^2}} \\
 & = \frac{73}{12}\sqrt{1 - \frac{1}{73^2}} \\ 
 & \approx \frac{73}{12}\left(1 - \frac{1}{2\cdot73^2}\right)
\end{align}


\\[ \hat{\pi}_{MAP} = \arg\max_\pi \Pr(\mathcal{X} | \pi) \Pr(\pi) \\]

